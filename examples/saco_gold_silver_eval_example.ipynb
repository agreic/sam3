{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b89e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from sam3.eval.cgf1_eval import CGF1Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceba210-cb61-4998-a153-c13c612e6182",
   "metadata": {},
   "source": [
    "# SA-Co/Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ab5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update to the directory where the GT annotation and PRED files exist\n",
    "GT_DIR = # PUT YOUR PATH HERE\n",
    "PRED_DIR = # PUT YOUR PATH HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25613248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative file names for GT files for 7 SA-Co/Gold subsets\n",
    "saco_gold_gts = {\n",
    "    # MetaCLIP Captioner\n",
    "    \"metaclip_nps\": [\n",
    "            \"gold_metaclip_merged_a_release_test.json\",\n",
    "            \"gold_metaclip_merged_b_release_test.json\",\n",
    "            \"gold_metaclip_merged_c_release_test.json\",\n",
    "    ],\n",
    "    # SA-1B captioner\n",
    "    \"sa1b_nps\": [\n",
    "            \"gold_sa1b_merged_a_release_test.json\",\n",
    "            \"gold_sa1b_merged_b_release_test.json\",\n",
    "            \"gold_sa1b_merged_c_release_test.json\",\n",
    "    ],\n",
    "    # Crowded\n",
    "    \"crowded\": [\n",
    "            \"gold_crowded_merged_a_release_test.json\",\n",
    "            \"gold_crowded_merged_b_release_test.json\",\n",
    "            \"gold_crowded_merged_c_release_test.json\",\n",
    "    ],\n",
    "    # FG Food\n",
    "    \"fg_food\": [\n",
    "            \"gold_fg_food_merged_a_release_test.json\",\n",
    "            \"gold_fg_food_merged_b_release_test.json\",\n",
    "            \"gold_fg_food_merged_c_release_test.json\",\n",
    "    ],\n",
    "    # FG Sports\n",
    "    \"fg_sports_equipment\": [\n",
    "            \"gold_fg_sports_equipment_merged_a_release_test.json\",\n",
    "            \"gold_fg_sports_equipment_merged_b_release_test.json\",\n",
    "            \"gold_fg_sports_equipment_merged_c_release_test.json\",\n",
    "    ],\n",
    "    # Attributes\n",
    "    \"attributes\": [\n",
    "            \"gold_attributes_merged_a_release_test.json\",\n",
    "            \"gold_attributes_merged_b_release_test.json\",\n",
    "            \"gold_attributes_merged_c_release_test.json\",\n",
    "    ],\n",
    "    # Wiki common\n",
    "    \"wiki_common\": [\n",
    "            \"gold_wiki_common_merged_a_release_test.json\",\n",
    "            \"gold_wiki_common_merged_b_release_test.json\",\n",
    "            \"gold_wiki_common_merged_c_release_test.json\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703e989",
   "metadata": {},
   "source": [
    "## Run offline evaluation for all 7 SA-Co/Gold subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0314ddca-46e7-47fd-9f66-346c4f8baf96",
   "metadata": {},
   "source": [
    "We assume the inference has already been run for all 7 datasets. With the default configurations, the predictions are dumped in a predictable folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28d29f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_gold = {}\n",
    "results_gold_bbox = {}\n",
    "\n",
    "for subset_name, gts in saco_gold_gts.items():\n",
    "    print(\"Processing subset: \", subset_name)\n",
    "    gt_paths = [os.path.join(GT_DIR, gt) for gt in gts]\n",
    "    pred_path = os.path.join(PRED_DIR, f\"gold_{subset_name}/dumps/gold_{subset_name}/coco_predictions_segm.json\")\n",
    "    \n",
    "    evaluator = CGF1Evaluator(gt_path=gt_paths, verbose=True, iou_type=\"segm\") \n",
    "    summary = evaluator.evaluate(pred_path)\n",
    "    print(summary)\n",
    "\n",
    "    cur_results = {}\n",
    "    cur_results[\"cgf1\"] = summary[\"cgF1_eval_segm_cgF1\"] * 100\n",
    "    cur_results[\"il_mcc\"] = summary[\"cgF1_eval_segm_IL_MCC\"]\n",
    "    cur_results[\"pmf1\"] = summary[\"cgF1_eval_segm_positive_micro_F1\"] * 100\n",
    "    results_gold[subset_name] = cur_results\n",
    "\n",
    "    # Also eval bbox    \n",
    "    evaluator = CGF1Evaluator(gt_path=gt_paths, verbose=True, iou_type=\"bbox\") \n",
    "    summary = evaluator.evaluate(pred_path)\n",
    "    print(summary)\n",
    "\n",
    "    cur_results = {}\n",
    "    cur_results[\"cgf1\"] = summary[\"cgF1_eval_bbox_cgF1\"] * 100\n",
    "    cur_results[\"il_mcc\"] = summary[\"cgF1_eval_bbox_IL_MCC\"]\n",
    "    cur_results[\"pmf1\"] = summary[\"cgF1_eval_bbox_positive_micro_F1\"] * 100\n",
    "    results_gold_bbox[subset_name] = cur_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808a7cf-ecda-445a-a827-53a16dee4504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute averages\n",
    "METRICS = [\"cgf1\", \"il_mcc\", \"pmf1\"]\n",
    "avg_stats, avg_stats_bbox = {}, {}\n",
    "for key in METRICS:\n",
    "    avg_stats[key] = sum(res[key] for res in results_gold.values()) / len(results_gold)\n",
    "    avg_stats_bbox[key] = sum(res[key] for res in results_gold_bbox.values()) / len(results_gold_bbox)\n",
    "results_gold[\"Average\"] = avg_stats\n",
    "results_gold_bbox[\"Average\"] = avg_stats_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b1fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print segmentation results\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "row1, row2, row3 = \"\", \"\", \"\"\n",
    "for subset in results_gold:\n",
    "    row1 += f'<th colspan=\"3\" style=\"text-align:center;border-left-style:solid;border-left-width:1px\">{subset}</th>'\n",
    "    row2 += \"<th style='border-left-style:solid;border-left-width:1px'>\" + \"</th><th>\".join(METRICS) + \"</th>\"\n",
    "    row3 += \"<td style='border-left-style:solid;border-left-width:1px'>\" + \"</td><td>\".join([str(round(results_gold[subset][k], 2)) for k in METRICS])  + \"</td>\"\n",
    "\n",
    "display(HTML(\n",
    "   f\"<table><thead><tr>{row1}</tr><tr>{row2}</tr></thead><tbody><tr>{row3}</tr></tbody></table>\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c85735-4aea-436d-a233-6f18cff29147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print bbox detection results\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "row1, row2, row3 = \"\", \"\", \"\"\n",
    "for subset in results_gold:\n",
    "    row1 += f'<th colspan=\"3\" style=\"text-align:center;border-left-style:solid;border-left-width:1px\">{subset}</th>'\n",
    "    row2 += \"<th style='border-left-style:solid;border-left-width:1px'>\" + \"</th><th>\".join(METRICS) + \"</th>\"\n",
    "    row3 += \"<td style='border-left-style:solid;border-left-width:1px'>\" + \"</td><td>\".join([str(round(results_gold_bbox[subset][k], 2)) for k in METRICS])  + \"</td>\"\n",
    "\n",
    "display(HTML(\n",
    "   f\"<table><thead><tr>{row1}</tr><tr>{row2}</tr></thead><tbody><tr>{row3}</tr></tbody></table>\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79428e-9212-4e26-8122-3e82841447de",
   "metadata": {},
   "source": [
    "# SA-Co/Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55995b3b-1184-4d1f-b9bb-ad412eb734a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update to the directory where the GT annotation and PRED files exist\n",
    "GT_DIR =  # PUT YOUR PATH HERE\n",
    "PRED_DIR =  # PUT YOUR PATH HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd254a73-7272-4df1-90d1-b818f5a7c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "saco_silver_gts = {\n",
    "    \"bdd100k\": \"silver_bdd100k_merged_test.json\",\n",
    "    \"droid\": \"silver_droid_merged_test.json\",\n",
    "    \"ego4d\": \"silver_ego4d_merged_test.json\",\n",
    "    \"food_rec\": \"silver_food_rec_merged_test.json\",\n",
    "    \"geode\": \"silver_geode_merged_test.json\",\n",
    "    \"inaturalist\": \"silver_inaturalist_merged_test.json\",\n",
    "    \"nga_art\": \"silver_nga_art_merged_test.json\",\n",
    "    \"sav\": \"silver_sav_merged_test.json\",\n",
    "    \"yt1b\": \"silver_yt1b_merged_test.json\",\n",
    "    \"fathomnet\": \"silver_fathomnet_test.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736eabf-6e52-4f52-b4ab-111eaa490584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_silver = {}\n",
    "results_silver_bbox = {}\n",
    "\n",
    "for subset_name, gt in saco_silver_gts.items():\n",
    "    print(\"Processing subset: \", subset_name)\n",
    "    gt_path = os.path.join(GT_DIR, gt)\n",
    "    pred_path = os.path.join(PRED_DIR, f\"silver_{subset_name}/dumps/silver_{subset_name}/coco_predictions_segm.json\")\n",
    "    \n",
    "    evaluator = CGF1Evaluator(gt_path=gt_path, verbose=True, iou_type=\"segm\") \n",
    "    summary = evaluator.evaluate(pred_path)\n",
    "    print(summary)\n",
    "\n",
    "    cur_results = {}\n",
    "    cur_results[\"cgf1\"] = summary[\"cgF1_eval_segm_cgF1\"] * 100\n",
    "    cur_results[\"il_mcc\"] = summary[\"cgF1_eval_segm_IL_MCC\"]\n",
    "    cur_results[\"pmf1\"] = summary[\"cgF1_eval_segm_positive_micro_F1\"] * 100\n",
    "    results_silver[subset_name] = cur_results\n",
    "\n",
    "    # Also eval bbox    \n",
    "    evaluator = CGF1Evaluator(gt_path=gt_path, verbose=True, iou_type=\"bbox\") \n",
    "    summary = evaluator.evaluate(pred_path)\n",
    "    print(summary)\n",
    "\n",
    "    cur_results = {}\n",
    "    cur_results[\"cgf1\"] = summary[\"cgF1_eval_bbox_cgF1\"] * 100\n",
    "    cur_results[\"il_mcc\"] = summary[\"cgF1_eval_bbox_IL_MCC\"]\n",
    "    cur_results[\"pmf1\"] = summary[\"cgF1_eval_bbox_positive_micro_F1\"] * 100\n",
    "    results_silver_bbox[subset_name] = cur_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e949a-05ad-46e4-9b97-78b36a5f8c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute averages\n",
    "METRICS = [\"cgf1\", \"il_mcc\", \"pmf1\"]\n",
    "avg_stats, avg_stats_bbox = {}, {}\n",
    "for key in METRICS:\n",
    "    avg_stats[key] = sum(res[key] for res in results_silver.values()) / len(results_silver)\n",
    "    avg_stats_bbox[key] = sum(res[key] for res in results_silver_bbox.values()) / len(results_silver_bbox)\n",
    "results_silver[\"Average\"] = avg_stats\n",
    "results_silver_bbox[\"Average\"] = avg_stats_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791eefb4-5e36-4dc0-9f26-2870355a7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print segmentation results\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "row1, row2, row3 = \"\", \"\", \"\"\n",
    "for subset in results_silver:\n",
    "    row1 += f'<th colspan=\"3\" style=\"text-align:center;border-left-style:solid;border-left-width:1px\">{subset}</th>'\n",
    "    row2 += \"<th style='border-left-style:solid;border-left-width:1px'>\" + \"</th><th>\".join(METRICS) + \"</th>\"\n",
    "    row3 += \"<td style='border-left-style:solid;border-left-width:1px'>\" + \"</td><td>\".join([str(round(results_silver[subset][k], 2)) for k in METRICS])  + \"</td>\"\n",
    "\n",
    "display(HTML(\n",
    "   f\"<table><thead><tr>{row1}</tr><tr>{row2}</tr></thead><tbody><tr>{row3}</tr></tbody></table>\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250c2da-fe9a-4c13-be32-edb54748440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print bbox detection results\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "row1, row2, row3 = \"\", \"\", \"\"\n",
    "for subset in results_silver_bbox:\n",
    "    row1 += f'<th colspan=\"3\" style=\"text-align:center;border-left-style:solid;border-left-width:1px\">{subset}</th>'\n",
    "    row2 += \"<th style='border-left-style:solid;border-left-width:1px'>\" + \"</th><th>\".join(METRICS) + \"</th>\"\n",
    "    row3 += \"<td style='border-left-style:solid;border-left-width:1px'>\" + \"</td><td>\".join([str(round(results_silver_bbox[subset][k], 2)) for k in METRICS])  + \"</td>\"\n",
    "\n",
    "display(HTML(\n",
    "   f\"<table><thead><tr>{row1}</tr><tr>{row2}</tr></thead><tbody><tr>{row3}</tr></tbody></table>\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059d9e2-ce61-42f9-9b12-3c61b3bf75bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
