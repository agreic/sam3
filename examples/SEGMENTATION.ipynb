{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image segmentation with SAM 3\n",
    "\n",
    "This notebook demonstrates how to use SAM 3 for image segmentation with text or visual prompts. It covers the following capabilities:\n",
    "\n",
    "- **Text prompts**: Using natural language descriptions to segment objects (e.g., \"person\", \"face\")\n",
    "- **Box prompts**: Using bounding boxes as exemplar visual prompts\n",
    "\n",
    "Will use for per frame segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 1389103906143178,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sam3\n",
    "from PIL import Image\n",
    "from sam3 import build_sam3_image_model\n",
    "from sam3.model.box_ops import box_xywh_to_cxcywh\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "from sam3.visualization_utils import draw_box_on_image, normalize_bbox, plot_results\n",
    "\n",
    "sam3_root = os.path.join(os.path.dirname(sam3.__file__), \"..\")\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from sam3.train.data.collator import collate_fn_api as collate\n",
    "from sam3.model.utils.misc import copy_data_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# turn on tfloat32 for Ampere GPUs\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# use bfloat16 for the entire notebook. If your card doesn't support it, try float16 instead\n",
    "torch.autocast(\"cuda\", dtype=torch.float16).__enter__()\n",
    "\n",
    "# inference mode for the whole notebook. Disable if you need gradients\n",
    "torch.inference_mode().__enter__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 785101161160169,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "# bpe_path = f\"{sam3_root}/assets/bpe_simple_vocab_16e6.txt.gz\"\n",
    "model = build_sam3_image_model(bpe_path=r\"/workspaces/sam3-main/sam3/assets/bpe_simple_vocab_16e6.txt.gz\", checkpoint_path=r\"/workspaces/sam3-main/assets/models/sam3.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam3.eval.postprocessors import PostProcessImage\n",
    "\n",
    "postprocessor = PostProcessImage(\n",
    "    max_dets_per_img=-1,\n",
    "    iou_type=\"segm\",\n",
    "    use_original_sizes_box=True,\n",
    "    use_original_sizes_mask=True,\n",
    "    convert_mask_to_rle=False,\n",
    "    detection_threshold=0.65,\n",
    "    to_cpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "# from PIL import Image as PILImage\n",
    "\n",
    "# class CLAHETransformAPI:\n",
    "#     def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "#         self.clip_limit = clip_limit\n",
    "#         self.tile_grid_size = tile_grid_size\n",
    "\n",
    "#     def __call__(self, datapoint, **kwargs):\n",
    "#         # 1. Ensure we have images\n",
    "#         if not hasattr(datapoint, 'images') or not datapoint.images:\n",
    "#             return datapoint\n",
    "\n",
    "#         for img_wrapper in datapoint.images:\n",
    "#             # 2. Extract PIL Image\n",
    "#             # We check the most likely locations for the actual bitmap\n",
    "#             actual_img = None\n",
    "#             if hasattr(img_wrapper, 'image'):\n",
    "#                 actual_img = img_wrapper.image\n",
    "#             elif hasattr(img_wrapper, '_image'):\n",
    "#                 actual_img = img_wrapper._image\n",
    "            \n",
    "#             if actual_img is None:\n",
    "#                 continue\n",
    "\n",
    "#             # 3. Process with CLAHE\n",
    "#             img_np = np.array(actual_img)\n",
    "            \n",
    "#             # Normalize 16-bit to 8-bit\n",
    "#             if img_np.dtype == np.uint16:\n",
    "#                 img_min, img_max = img_np.min(), img_np.max()\n",
    "#                 img_np = (255.0 * (img_np - img_min) / (img_max - img_min + 1e-6)).astype(np.uint8)\n",
    "            \n",
    "#             # CLAHE Logic\n",
    "#             if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n",
    "#                 lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
    "#                 l, a, b = cv2.split(lab)\n",
    "#                 clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
    "#                 l = clahe.apply(l)\n",
    "#                 img_np = cv2.cvtColor(cv2.merge((l, a, b)), cv2.COLOR_LAB2RGB)\n",
    "#             else:\n",
    "#                 if img_np.ndim == 3: img_np = img_np.squeeze()\n",
    "#                 clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
    "#                 img_np = clahe.apply(img_np)\n",
    "#                 img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "#             # 4. Update the wrapper\n",
    "#             new_pil = PILImage.fromarray(img_np)\n",
    "#             if hasattr(img_wrapper, 'image'):\n",
    "#                 img_wrapper.image = new_pil\n",
    "#             elif hasattr(img_wrapper, '_image'):\n",
    "#                 img_wrapper._image = new_pil\n",
    "            \n",
    "#             # CRITICAL: Some SAM3 wrappers have a 'tensor' or '_tensor' attribute \n",
    "#             # that needs to be cleared so 'ToTensorAPI' regenerates it.\n",
    "#             for tensor_attr in ['tensor', '_tensor', 'data', '_data']:\n",
    "#                 if hasattr(img_wrapper, tensor_attr):\n",
    "#                     setattr(img_wrapper, tensor_attr, None)\n",
    "\n",
    "#         return datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam3.train.transforms.basic_for_api import ComposeAPI, RandomResizeAPI, ToTensorAPI, NormalizeAPI, CLAHETransformAPI\n",
    "\n",
    "# transform = ComposeAPI(\n",
    "#     transforms=[\n",
    "#         RandomResizeAPI(sizes=1008, max_size=1008, square=True, consistent_transform=False),\n",
    "#         # RandomResizeAPI(sizes=1152, max_size=1152, square=True, consistent_transform=False),\n",
    "#         ToTensorAPI(),\n",
    "#         NormalizeAPI(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "transform = ComposeAPI(\n",
    "    transforms=[\n",
    "        # 1. Enhance features first\n",
    "        CLAHETransformAPI(clip_limit=3.0),\n",
    "        \n",
    "        # 2. Resize to the mandatory 1008 (avoiding the RoPE error)\n",
    "        RandomResizeAPI(sizes=1008, max_size=1008, square=True, consistent_transform=False),\n",
    "        \n",
    "        # 3. Convert to Tensor\n",
    "        ToTensorAPI(),\n",
    "        \n",
    "        # 4. Standard SAM3 Normalization\n",
    "        NormalizeAPI(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam3.train.data.sam3_image_dataset import InferenceMetadata, FindQueryLoaded, Image as SAMImage, Datapoint\n",
    "from typing import List\n",
    "\n",
    "GLOBAL_COUNTER = 1\n",
    "\n",
    "def create_empty_datapoint():\n",
    "    \"\"\" A datapoint is a single image on which we can apply several queries at once. \"\"\"\n",
    "    return Datapoint(find_queries=[], images=[])\n",
    "\n",
    "def set_image(datapoint, pil_image):\n",
    "    \"\"\" Add the image to be processed to the datapoint \"\"\"\n",
    "    w, h = pil_image.size\n",
    "    datapoint.images = [SAMImage(data=pil_image, objects=[], size=[h, w])]\n",
    "\n",
    "def add_text_prompt(datapoint, text_query):\n",
    "    \"\"\" Add a text query to the datapoint \"\"\"\n",
    "    global GLOBAL_COUNTER\n",
    "    assert len(datapoint.images) == 1, \"please set the image first\"\n",
    "    \n",
    "    w, h = datapoint.images[0].size\n",
    "    datapoint.find_queries.append(\n",
    "        FindQueryLoaded(\n",
    "            query_text=text_query,\n",
    "            image_id=0,\n",
    "            object_ids_output=[],\n",
    "            is_exhaustive=True,\n",
    "            query_processing_order=0,\n",
    "            inference_metadata=InferenceMetadata(\n",
    "                coco_image_id=GLOBAL_COUNTER,\n",
    "                original_image_id=GLOBAL_COUNTER,\n",
    "                original_category_id=1,\n",
    "                original_size=[w, h],\n",
    "                object_id=0,\n",
    "                frame_index=0,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    GLOBAL_COUNTER += 1\n",
    "    return GLOBAL_COUNTER - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Batching Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "folder_path = \"/root/data/230624DS30/230624DS30_p0001\"\n",
    "output_folder = \"/root/data/Segmentation_SAM3/230624DS30/230624DS30_p0001\"\n",
    "text_prompt = \"cells\"  # What to segment\n",
    "batch_size = 2  # Number of images to process at once\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get all image files\n",
    "image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.tif', '*.tiff', '*.bmp']\n",
    "image_files = []\n",
    "for ext in image_extensions:\n",
    "    image_files.extend(glob.glob(os.path.join(folder_path, ext)))\n",
    "    image_files.extend(glob.glob(os.path.join(folder_path, ext.upper())))\n",
    "\n",
    "image_files = sorted(image_files)\n",
    "image_files = [i for i in image_files if \"w00\" in i]\n",
    "\n",
    "# only use last 20\n",
    "image_files = image_files[-4:]\n",
    "\n",
    "# print first few names\n",
    "for i in range(4):\n",
    "    print(image_files[i])\n",
    "\n",
    "\n",
    "print(f\"Found {len(image_files)} images to process\")\n",
    "print(f\"Output will be saved to: {output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Process All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# # Process images in batches\n",
    "# for batch_idx in tqdm(range(0, len(image_files), batch_size), desc=\"Processing batches\"):\n",
    "#     batch_files = image_files[batch_idx:batch_idx + batch_size]\n",
    "    \n",
    "#     # Create datapoints for this batch\n",
    "#     datapoints = []\n",
    "#     query_ids = []\n",
    "#     images = []\n",
    "    \n",
    "#     for img_path in batch_files:\n",
    "#         img = Image.open(img_path)\n",
    "\n",
    "#         # Only take the size that the model allows (1008x)\n",
    "#         img = i\n",
    "        \n",
    "#         # Convert to RGB if needed (handles grayscale and 16-bit images)\n",
    "#         if img.mode != 'RGB':\n",
    "#             # For 16-bit or grayscale images, convert to 8-bit RGB\n",
    "#             if img.mode == 'I;16' or img.mode == 'I':\n",
    "#                 # Normalize 16-bit to 8-bit range\n",
    "#                 img_array = np.array(img)\n",
    "#                 img_array = ((img_array - img_array.min()) / (img_array.max() - img_array.min()) * 255).astype(np.uint8)\n",
    "#                 img = Image.fromarray(img_array)\n",
    "#             img = img.convert('RGB')\n",
    "        \n",
    "#         images.append(img)\n",
    "        \n",
    "#         datapoint = create_empty_datapoint()\n",
    "#         set_image(datapoint, img)\n",
    "#         query_id = add_text_prompt(datapoint, text_prompt)\n",
    "#         query_ids.append(query_id)\n",
    "        \n",
    "#         datapoint = transform(datapoint)\n",
    "#         datapoints.append(datapoint)\n",
    "    \n",
    "#     # Collate and move to GPU\n",
    "#     batch = collate(datapoints, dict_key=\"dummy\")[\"dummy\"]\n",
    "#     batch = copy_data_to_device(batch, torch.device(\"cuda\"), non_blocking=True)\n",
    "    \n",
    "#     # Forward pass\n",
    "#     output = model(batch)\n",
    "    \n",
    "#     # Post-process results\n",
    "#     processed_results = postprocessor.process_results(output, batch.find_metadatas)\n",
    "    \n",
    "#     # Save results for each image\n",
    "#     for i, (img_path, query_id, img) in enumerate(zip(batch_files, query_ids, images)):\n",
    "#         result = processed_results[query_id]\n",
    "        \n",
    "#         # Get base filename without extension\n",
    "#         base_name = Path(img_path).stem\n",
    "        \n",
    "#         # Save masks as single image with IDs\n",
    "#         if \"masks\" in result and len(result[\"masks\"]) > 0:\n",
    "#             masks = result[\"masks\"]\n",
    "            \n",
    "#             # Convert to numpy if it's a tensor\n",
    "#             if torch.is_tensor(masks):\n",
    "#                 masks = masks.cpu().numpy()\n",
    "            \n",
    "#             height, width = img.size[1], img.size[0]\n",
    "            \n",
    "#             # Handle different mask shapes\n",
    "#             if masks.ndim == 4:\n",
    "#                 masks = masks.squeeze(1)\n",
    "            \n",
    "#             mask_image = np.zeros((height, width), dtype=np.uint16)\n",
    "#             for mask_idx in range(masks.shape[0]):\n",
    "#                 mask_image[masks[mask_idx] > 0.5] = mask_idx + 1\n",
    "            \n",
    "#             mask_pil = Image.fromarray(mask_image)\n",
    "#             mask_pil.save(os.path.join(output_folder, f\"{base_name}_masks.png\"))\n",
    "        \n",
    "#         # Save boxes\n",
    "#         if \"boxes\" in result and len(result[\"boxes\"]) > 0:\n",
    "#             boxes = result[\"boxes\"]\n",
    "#             # Convert to numpy if it's a tensor\n",
    "#             if torch.is_tensor(boxes):\n",
    "#                 boxes = boxes.cpu().numpy()\n",
    "#             np.savetxt(os.path.join(output_folder, f\"{base_name}_boxes.txt\"), boxes, fmt=\"%.2f\")\n",
    "        \n",
    "#         # Save scores\n",
    "#         if \"scores\" in result and len(result[\"scores\"]) > 0:\n",
    "#             scores = result[\"scores\"]\n",
    "#             # Convert to numpy if it's a tensor\n",
    "#             if torch.is_tensor(scores):\n",
    "#                 scores = scores.cpu().numpy()\n",
    "#             np.savetxt(os.path.join(output_folder, f\"{base_name}_scores.txt\"), scores, fmt=\"%.4f\")\n",
    "#             print(os.path.join(output_folder, f\"{base_name}_scores.txt\"))\n",
    "\n",
    "# print(f\"Processing complete! Results saved to {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- Configuration for 2304x2304 Input ---\n",
    "TILE_SIZE = 1008\n",
    "# Stride calculation: (2304 - 1008) / 2 = 648. \n",
    "# This gives exactly 3 tiles: [0, 648, 1296] (last one ends at 2304)\n",
    "STRIDES = [0, 648, 2304 - TILE_SIZE] \n",
    "\n",
    "print(f\"Starting Tiled Inference: {len(image_files)} images, Tile Size={TILE_SIZE}, Grid=3x3\")\n",
    "\n",
    "# We process the tiles in batches, not the full images, to keep memory low\n",
    "# Flatten all tasks: specific image path + specific tile coordinates\n",
    "all_tile_tasks = []\n",
    "for img_path in image_files:\n",
    "    base_name = Path(img_path).stem\n",
    "    for top in STRIDES:\n",
    "        for left in STRIDES:\n",
    "            all_tile_tasks.append({\n",
    "                \"path\": img_path,\n",
    "                \"base_name\": base_name,\n",
    "                \"top\": top,\n",
    "                \"left\": left\n",
    "            })\n",
    "\n",
    "# Dictionary to hold results before stitching\n",
    "# Structure: { \"filename\": { \"boxes\": [], \"scores\": [], \"masks\": [] } }\n",
    "stitched_results = {}\n",
    "\n",
    "# Process tiles in chunks\n",
    "for i in tqdm(range(0, len(all_tile_tasks), batch_size), desc=\"Processing Tiles\"):\n",
    "    batch_tasks = all_tile_tasks[i : i + batch_size]\n",
    "    \n",
    "    datapoints = []\n",
    "    metadata_batch = []\n",
    "    \n",
    "    # 1. Prepare Batch\n",
    "    for task in batch_tasks:\n",
    "        # Load Full Image (Lazy load could be optimized, but OS caching helps here)\n",
    "        full_img = Image.open(task[\"path\"])\n",
    "        \n",
    "        # Crop the 1008x1008 tile\n",
    "        tile = full_img.crop((task[\"left\"], task[\"top\"], task[\"left\"] + TILE_SIZE, task[\"top\"] + TILE_SIZE))\n",
    "        \n",
    "        # Standardize 16-bit/Grayscale to RGB\n",
    "        # (Your CLAHE transform will handle normalization/contrast later if configured)\n",
    "        if tile.mode != 'RGB':\n",
    "            if tile.mode in ['I;16', 'I']:\n",
    "                arr = np.array(tile)\n",
    "                # Normalize 16-bit to 8-bit\n",
    "                arr = ((arr - arr.min()) / (arr.max() - arr.min() + 1e-6) * 255).astype(np.uint8)\n",
    "                tile = Image.fromarray(arr)\n",
    "            tile = tile.convert('RGB')\n",
    "\n",
    "        # Construct Datapoint\n",
    "        dp = create_empty_datapoint()\n",
    "        set_image(dp, tile)\n",
    "        qid = add_text_prompt(dp, text_prompt)\n",
    "        \n",
    "        # Transform (Apply CLAHE, Resizing, Normalization here)\n",
    "        # Ensure your transform is set to output 1008x1008 tensors!\n",
    "        dp = transform(dp)\n",
    "        \n",
    "        datapoints.append(dp)\n",
    "        metadata_batch.append({\n",
    "            \"base_name\": task[\"base_name\"],\n",
    "            \"offset\": torch.tensor([task[\"left\"], task[\"top\"], task[\"left\"], task[\"top\"]]),\n",
    "            \"query_id\": qid\n",
    "        })\n",
    "\n",
    "    # 2. Inference\n",
    "    if not datapoints: continue\n",
    "    \n",
    "    batch_input = collate(datapoints, dict_key=\"dummy\")[\"dummy\"]\n",
    "    batch_input = copy_data_to_device(batch_input, torch.device(\"cuda\"), non_blocking=True)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        output = model(batch_input)\n",
    "    \n",
    "# 3. Process & Collect Results with EDGE FILTERING\n",
    "    processed_batch = postprocessor.process_results(output, batch_input.find_metadatas)\n",
    "    if isinstance(processed_batch, dict):\n",
    "        results_list = list(processed_batch.values())\n",
    "    else:\n",
    "        results_list = processed_batch\n",
    "\n",
    "    for i, meta in enumerate(metadata_batch):\n",
    "        if i >= len(results_list): continue\n",
    "        res = results_list[i]\n",
    "        name = meta[\"base_name\"]\n",
    "        \n",
    "        if name not in stitched_results:\n",
    "            stitched_results[name] = {\"boxes\": [], \"scores\": [], \"masks\": [], \"mask_offsets\": []}\n",
    "        \n",
    "        if \"boxes\" in res and len(res[\"boxes\"]) > 0:\n",
    "            boxes = res[\"boxes\"] # Local coordinates (0-1008)\n",
    "            scores = res[\"scores\"]\n",
    "            masks = res[\"masks\"]\n",
    "            \n",
    "            # --- EDGE REJECTION LOGIC ---\n",
    "            # Define inner margins (e.g., 5-10 pixels from the tile edge)\n",
    "            margin = 5\n",
    "            tile_h, tile_w = TILE_SIZE, TILE_SIZE # 1008\n",
    "            \n",
    "            # Check which edges of the tile are ACTUAL image borders\n",
    "            # meta[\"offset\"] is [left, top, left, top]\n",
    "            # If left == 0, we keep left-touching cells. If left > 0, we drop them.\n",
    "            is_left_edge = (meta[\"offset\"][0] == 0)\n",
    "            is_top_edge = (meta[\"offset\"][1] == 0)\n",
    "            # Check if this tile reaches the bottom/right of the full 2304 image\n",
    "            is_right_edge = (meta[\"offset\"][0] + TILE_SIZE >= 2304)\n",
    "            is_bottom_edge = (meta[\"offset\"][1] + TILE_SIZE >= 2304)\n",
    "\n",
    "            keep_mask = torch.ones(len(boxes), dtype=torch.bool, device=boxes.device)\n",
    "            \n",
    "            # x1, y1, x2, y2\n",
    "            b_x1 = boxes[:, 0]\n",
    "            b_y1 = boxes[:, 1]\n",
    "            b_x2 = boxes[:, 2]\n",
    "            b_y2 = boxes[:, 3]\n",
    "\n",
    "            # Filter Left: If not true image edge, drop boxes touching x=0\n",
    "            if not is_left_edge:\n",
    "                keep_mask &= (b_x1 > margin)\n",
    "            \n",
    "            # Filter Top: If not true image edge, drop boxes touching y=0\n",
    "            if not is_top_edge:\n",
    "                keep_mask &= (b_y1 > margin)\n",
    "            \n",
    "            # Filter Right: If not true image edge, drop boxes touching x=1008\n",
    "            if not is_right_edge:\n",
    "                keep_mask &= (b_x2 < tile_w - margin)\n",
    "                \n",
    "            # Filter Bottom: If not true image edge, drop boxes touching y=1008\n",
    "            if not is_bottom_edge:\n",
    "                keep_mask &= (b_y2 < tile_h - margin)\n",
    "            \n",
    "            # Apply Filter\n",
    "            if keep_mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            boxes = boxes[keep_mask]\n",
    "            scores = scores[keep_mask]\n",
    "            masks = masks[keep_mask]\n",
    "            \n",
    "            # Shift boxes to Global Coordinates\n",
    "            offset = meta[\"offset\"].to(boxes.device)\n",
    "            global_boxes = boxes + offset\n",
    "            \n",
    "            stitched_results[name][\"boxes\"].append(global_boxes)\n",
    "            stitched_results[name][\"scores\"].append(scores)\n",
    "            stitched_results[name][\"masks\"].append(masks)\n",
    "            \n",
    "            # Store offsets for reconstruction\n",
    "            n_dets = len(scores)\n",
    "            stitched_results[name][\"mask_offsets\"].append(offset[:2].repeat(n_dets, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stitching & Saving Phase ---\n",
    "print(\"Stitching tiles and saving results...\")\n",
    "\n",
    "for base_name, data in tqdm(stitched_results.items(), desc=\"Saving Images\"):\n",
    "    if not data[\"boxes\"]:\n",
    "        print(f\"No detections for {base_name}\")\n",
    "        continue\n",
    "        \n",
    "    # 1. Concatenate all tiles for this image\n",
    "    # We use .detach() to ensure we aren't dragging the gradient graph along\n",
    "    all_boxes = torch.cat(data[\"boxes\"]).detach()\n",
    "    all_scores = torch.cat(data[\"scores\"]).detach()\n",
    "    all_masks = torch.cat(data[\"masks\"]).detach()\n",
    "    all_offsets = torch.cat(data[\"mask_offsets\"]).detach()\n",
    "\n",
    "    # 2. CRITICAL FIX: Cast to float32 to satisfy torchvision.ops.nms\n",
    "    all_boxes = all_boxes.to(dtype=torch.float32)\n",
    "    all_scores = all_scores.to(dtype=torch.float32)\n",
    "\n",
    "    # 3. Global NMS (Non-Maximum Suppression)\n",
    "    # Remove duplicates in overlap regions (IoU > 0.5)\n",
    "    keep_indices = torchvision.ops.nms(all_boxes, all_scores, iou_threshold=0.5)\n",
    "\n",
    "    # Change from 0.5 to 0.3 or 0.2\n",
    "    # This prevents merging two different cells that are packed tightly together\n",
    "    # keep_indices = torchvision.ops.nms(all_boxes, all_scores, iou_threshold=0.3)\n",
    "    \n",
    "    # 4. Filter results based on NMS\n",
    "    final_boxes = all_boxes[keep_indices].cpu().numpy()\n",
    "    final_scores = all_scores[keep_indices].cpu().numpy()\n",
    "    final_masks = all_masks[keep_indices] # Keep on GPU for faster processing\n",
    "    final_offsets = all_offsets[keep_indices]\n",
    "\n",
    "    # 5. Reconstruct the 2304x2304 Mask Image\n",
    "    # 0 = Background, 1..N = Instance IDs\n",
    "    # We use int32 to prevent overflow if you have > 65535 cells (unlikely but safe)\n",
    "    full_mask = torch.zeros((2304, 2304), dtype=torch.int32, device=\"cuda\")\n",
    "    \n",
    "    for i, (mask_tensor, offset) in enumerate(zip(final_masks, final_offsets)):\n",
    "        # Squeeze if mask is (1, H, W) -> (H, W)\n",
    "        if mask_tensor.ndim == 3: \n",
    "            mask_tensor = mask_tensor.squeeze(0)\n",
    "        \n",
    "        # Threshold logic\n",
    "        if mask_tensor.min() < 0:\n",
    "            binary_mask = mask_tensor > 0.0\n",
    "        else:\n",
    "            binary_mask = mask_tensor > 0.5\n",
    "        \n",
    "        # Calculate placement coordinates\n",
    "        x_start, y_start = int(offset[0]), int(offset[1])\n",
    "        x_end = int(x_start + TILE_SIZE)\n",
    "        y_end = int(y_start + TILE_SIZE)\n",
    "        \n",
    "        # Clip coordinates to image boundaries\n",
    "        img_h, img_w = 2304, 2304\n",
    "        x_end = min(x_end, img_w)\n",
    "        y_end = min(y_end, img_h)\n",
    "        \n",
    "        # Calculate mask dimensions\n",
    "        mask_h = y_end - y_start\n",
    "        mask_w = x_end - x_start\n",
    "        \n",
    "        # Slice the binary mask\n",
    "        valid_binary_mask = binary_mask[:mask_h, :mask_w]\n",
    "        \n",
    "        # --- CRITICAL FIX: DEVICE SYNC ---\n",
    "        # Ensure the mask slice is on the same device as the full_mask (GPU)\n",
    "        valid_binary_mask = valid_binary_mask.to(full_mask.device)\n",
    "        \n",
    "        # Get the region of the full mask\n",
    "        current_slice = full_mask[y_start:y_end, x_start:x_end]\n",
    "        \n",
    "        # Perform the update\n",
    "        # We ensure the scalar 'i+1' is also on the correct device\n",
    "        full_mask[y_start:y_end, x_start:x_end] = torch.where(\n",
    "            valid_binary_mask, \n",
    "            torch.tensor(i + 1, dtype=torch.int32, device=full_mask.device), \n",
    "            current_slice\n",
    "        )\n",
    "\n",
    "    # 6. Save Files\n",
    "    # Move final mask to CPU/Numpy for saving\n",
    "    full_mask_np = full_mask.cpu().numpy().astype(np.uint16)\n",
    "\n",
    "    # Save Mask (Use PIL or OpenCV)\n",
    "    Image.fromarray(full_mask_np).save(os.path.join(output_folder, f\"{base_name}_masks.png\"))\n",
    "    \n",
    "    # Save Boxes\n",
    "    np.savetxt(os.path.join(output_folder, f\"{base_name}_boxes.txt\"), final_boxes, fmt=\"%.2f\")\n",
    "    \n",
    "    # Save Scores\n",
    "    np.savetxt(os.path.join(output_folder, f\"{base_name}_scores.txt\"), final_scores, fmt=\"%.4f\")\n",
    "\n",
    "print(f\"Processing complete! Results saved to {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "\n",
    "# def optimized_prepare_batch(img_paths):\n",
    "#     tensors = []\n",
    "#     for path in img_paths:\n",
    "#         # Load as-is (16-bit)\n",
    "#         img = Image.open(path)\n",
    "#         arr = np.array(img).astype(np.float32)\n",
    "        \n",
    "#         # Normalize 0-65535 to 0-1 on CPU quickly\n",
    "#         arr /= 65535.0 \n",
    "        \n",
    "#         # To tensor (H, W) -> (1, H, W)\n",
    "#         t = torch.from_numpy(arr).unsqueeze(0)\n",
    "#         tensors.append(t)\n",
    "    \n",
    "#     # Stack into (B, 1, H, W)\n",
    "#     batch = torch.stack(tensors).to(\"cuda\", non_blocking=True)\n",
    "    \n",
    "#     # \"Triple\" the data ON THE GPU using .expand\n",
    "#     # This creates a view, not a memory copy, until necessary\n",
    "#     batch_rgb = batch.expand(-1, 3, -1, -1) \n",
    "#     return batch_rgb\n",
    "\n",
    "# # Updated Benchmark for 2304x2304\n",
    "# def benchmark_sam3_real_res(model, test_batch_sizes=[1, 2, 4, 8]):\n",
    "#     # Note: 2304x2304 is huge, you likely won't get past batch 4 or 8\n",
    "#     for b in test_batch_sizes:\n",
    "#         try:\n",
    "#             # Match your actual input resolution\n",
    "#             dummy_batch = torch.randn(b, 3, 2304, 2304).to(\"cuda\")\n",
    "            \n",
    "#             with torch.inference_mode():\n",
    "#                 # Warmup\n",
    "#                 _ = model(dummy_batch)\n",
    "                \n",
    "#                 torch.cuda.synchronize()\n",
    "#                 start = time.perf_counter()\n",
    "#                 for _ in range(5):\n",
    "#                     _ = model(dummy_batch)\n",
    "#                 torch.cuda.synchronize()\n",
    "                \n",
    "#                 throughput = (b * 5) / (time.perf_counter() - start)\n",
    "#                 print(f\"BS: {b} | {throughput:.2f} images/sec\")\n",
    "#         except torch.cuda.OutOfMemoryError:\n",
    "#             print(f\"BS: {b} | OOM\")\n",
    "#             torch.cuda.empty_cache()\n",
    "#             break\n",
    "\n",
    "# benchmark_sam3_real_res(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "# Visualize results for a sample image\n",
    "if len(image_files) > 0:\n",
    "    sample_idx = -1\n",
    "    sample_path = image_files[sample_idx]\n",
    "    base_name = Path(sample_path).stem\n",
    "    \n",
    "    # Paths for all result files\n",
    "    mask_path = os.path.join(output_folder, f\"{base_name}_masks.png\")\n",
    "    box_path = os.path.join(output_folder, f\"{base_name}_boxes.txt\")\n",
    "    score_path = os.path.join(output_folder, f\"{base_name}_scores.txt\") # Load scores\n",
    "    \n",
    "    if os.path.exists(mask_path) and os.path.exists(box_path) and os.path.exists(score_path):\n",
    "        img = Image.open(sample_path)\n",
    "        mask_image = np.array(Image.open(mask_path))\n",
    "        \n",
    "        # Handle cases where files might be empty\n",
    "        boxes = np.loadtxt(box_path).reshape(-1, 4) if os.path.getsize(box_path) > 0 else np.array([])\n",
    "        scores = np.loadtxt(score_path).reshape(-1) if os.path.getsize(score_path) > 0 else np.array([])\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "        # Original Image + Bounding Boxes & Confidence\n",
    "        axes[0].imshow(img)\n",
    "        for i, (box, score) in enumerate(zip(boxes, scores)):\n",
    "            # Box format: assuming [x1, y1, x2, y2]\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "            axes[0].add_patch(rect)\n",
    "            \n",
    "            # Confidence Label\n",
    "            label = f\"{score*100:.1f}%\"\n",
    "            axes[0].text(x1, y1 - 10, label, color='white', fontsize=6, \n",
    "                         fontweight='bold', bbox=dict(facecolor='red', alpha=0.2, edgecolor='none'))\n",
    "            \n",
    "        axes[0].set_title(f\"Detections: {base_name}\")\n",
    "        axes[0].axis(\"off\")\n",
    "        \n",
    "        # Mask Visualization\n",
    "        axes[1].imshow(mask_image, cmap='nipy_spectral')\n",
    "        axes[1].set_title(f\"Masks: {len(scores)} objects found\")\n",
    "        axes[1].axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save image\n",
    "        plt.savefig(\"check.png\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Sample: {base_name}\")\n",
    "        print(f\"Confidence range: {scores.min():.2f} to {scores.max():.2f}\")\n",
    "    else:\n",
    "        print(f\"Missing result files for {base_name}. Check your output folder!\")\n",
    "else:\n",
    "    print(\"No images found in the folder!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Single Image Examples (Original Code)\n",
    "\n",
    "The cells below show the original single-image processing examples for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual prompt: a single bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 4344515869113686,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "# # Here the box is in  (x,y,w,h) format, where (x,y) is the top left corner.\n",
    "# box_input_xywh = torch.tensor([480.0, 290.0, 110.0, 360.0]).view(-1, 4)\n",
    "# box_input_cxcywh = box_xywh_to_cxcywh(box_input_xywh)\n",
    "\n",
    "# norm_box_cxcywh = normalize_bbox(box_input_cxcywh, width, height).flatten().tolist()\n",
    "# print(\"Normalized box input:\", norm_box_cxcywh)\n",
    "\n",
    "# processor.reset_all_prompts(inference_state)\n",
    "# inference_state = processor.add_geometric_prompt(\n",
    "#     state=inference_state, box=norm_box_cxcywh, label=True\n",
    "# )\n",
    "\n",
    "# img0 = Image.open(image_path)\n",
    "# image_with_box = draw_box_on_image(img0, box_input_xywh.flatten().tolist())\n",
    "# plt.imshow(image_with_box)\n",
    "# plt.axis(\"off\")  # Hide the axis\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 1972454793596108,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "# plot_results(img0, inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual prompt: multi-box prompting (with positive and negative boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 1157754466299886,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "# box_input_xywh = [[480.0, 290.0, 110.0, 360.0], [370.0, 280.0, 115.0, 375.0]]\n",
    "# box_input_cxcywh = box_xywh_to_cxcywh(torch.tensor(box_input_xywh).view(-1,4))\n",
    "# norm_boxes_cxcywh = normalize_bbox(box_input_cxcywh, width, height).tolist()\n",
    "\n",
    "# box_labels = [True, False]\n",
    "\n",
    "# processor.reset_all_prompts(inference_state)\n",
    "\n",
    "# for box, label in zip(norm_boxes_cxcywh, box_labels):\n",
    "#     inference_state = processor.add_geometric_prompt(\n",
    "#         state=inference_state, box=box, label=label\n",
    "#     )\n",
    "\n",
    "# img0 = Image.open(image_path)\n",
    "# image_with_box = img0\n",
    "# for i in range(len(box_input_xywh)):\n",
    "#     if box_labels[i] == 1:\n",
    "#         color = (0, 255, 0)\n",
    "#     else:\n",
    "#         color = (255, 0, 0)\n",
    "#     image_with_box = draw_box_on_image(image_with_box, box_input_xywh[i], color)\n",
    "# plt.imshow(image_with_box)\n",
    "# plt.axis(\"off\")  # Hide the axis\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 1352849976506927,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "# plot_results(img0, inference_state)"
   ]
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "ff722971-ca5d-431f-8450-ccfea4ff0708",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
