{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image segmentation with SAM 3\n",
    "\n",
    "This notebook demonstrates how to use SAM 3 for image segmentation with text or visual prompts. It covers the following capabilities:\n",
    "\n",
    "- **Text prompts**: Using natural language descriptions to segment objects (e.g., \"person\", \"face\")\n",
    "- **Box prompts**: Using bounding boxes as exemplar visual prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/sam3/blob/main/notebooks/sam3_image_predictor_example.ipynb\">\n",
    "#   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "# </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib scikit-learn\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam3.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 1389103906143178,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sam3\n",
    "from PIL import Image\n",
    "from sam3 import build_sam3_image_model\n",
    "from sam3.model.box_ops import box_xywh_to_cxcywh\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "from sam3.visualization_utils import draw_box_on_image, normalize_bbox, plot_results\n",
    "\n",
    "sam3_root = os.path.join(os.path.dirname(sam3.__file__), \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# turn on tfloat32 for Ampere GPUs\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# use bfloat16 for the entire notebook\n",
    "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 785101161160169,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "# bpe_path = f\"{sam3_root}/assets/bpe_simple_vocab_16e6.txt.gz\"\n",
    "model = build_sam3_image_model(bpe_path=r\"/workspaces/sam3-main/sam3/assets/bpe_simple_vocab_16e6.txt.gz\", checkpoint_path=r\"/workspaces/sam3-main/assets/models/sam3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = f\"/workspaces/sam3-main/assets/videos/251118YZ18_p0113/251118YZ18_p0113_t00001_z001_w00.png\"\n",
    "image = Image.open(image_path)\n",
    "width, height = image.size\n",
    "processor = Sam3Processor(model, confidence_threshold=0.65)\n",
    "inference_state = processor.set_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.reset_all_prompts(inference_state)\n",
    "inference_state = processor.set_text_prompt(state=inference_state, prompt=\"cell\")\n",
    "\n",
    "img0 = Image.open(image_path)\n",
    "plot_results(img0, inference_state)\n",
    "\n",
    "# Save the output masks as one image, with id as value (0 for background)\n",
    "masks = inference_state[\"masks\"].cpu().numpy()\n",
    "# Handle different mask shapes: could be (N, H, W) or (N, 1, H, W)\n",
    "if masks.ndim == 4:\n",
    "    masks = masks.squeeze(1)  # Remove channel dimension if present\n",
    "mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "for i in range(masks.shape[0]):\n",
    "    mask_image[masks[i] > 0.5] = i + 1  # Assign unique id to each mask\n",
    "mask_pil = Image.fromarray(mask_image)\n",
    "mask_pil.save(\"predicted_masks.png\")\n",
    "# # Print the full inference state\n",
    "# print(inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the boxes as a text file\n",
    "boxes = inference_state[\"boxes\"].cpu().numpy()\n",
    "np.savetxt(\"predicted_boxes.txt\", boxes, fmt=\"%.2f\")\n",
    "# Print the boxes\n",
    "print(\"Predicted boxes:\")\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual prompt: a single bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 4344515869113686,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "# Here the box is in  (x,y,w,h) format, where (x,y) is the top left corner.\n",
    "box_input_xywh = torch.tensor([480.0, 290.0, 110.0, 360.0]).view(-1, 4)\n",
    "box_input_cxcywh = box_xywh_to_cxcywh(box_input_xywh)\n",
    "\n",
    "norm_box_cxcywh = normalize_bbox(box_input_cxcywh, width, height).flatten().tolist()\n",
    "print(\"Normalized box input:\", norm_box_cxcywh)\n",
    "\n",
    "processor.reset_all_prompts(inference_state)\n",
    "inference_state = processor.add_geometric_prompt(\n",
    "    state=inference_state, box=norm_box_cxcywh, label=True\n",
    ")\n",
    "\n",
    "img0 = Image.open(image_path)\n",
    "image_with_box = draw_box_on_image(img0, box_input_xywh.flatten().tolist())\n",
    "plt.imshow(image_with_box)\n",
    "plt.axis(\"off\")  # Hide the axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 1972454793596108,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "plot_results(img0, inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual prompt: multi-box prompting (with positive and negative boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 1157754466299886,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "box_input_xywh = [[480.0, 290.0, 110.0, 360.0], [370.0, 280.0, 115.0, 375.0]]\n",
    "box_input_cxcywh = box_xywh_to_cxcywh(torch.tensor(box_input_xywh).view(-1,4))\n",
    "norm_boxes_cxcywh = normalize_bbox(box_input_cxcywh, width, height).tolist()\n",
    "\n",
    "box_labels = [True, False]\n",
    "\n",
    "processor.reset_all_prompts(inference_state)\n",
    "\n",
    "for box, label in zip(norm_boxes_cxcywh, box_labels):\n",
    "    inference_state = processor.add_geometric_prompt(\n",
    "        state=inference_state, box=box, label=label\n",
    "    )\n",
    "\n",
    "img0 = Image.open(image_path)\n",
    "image_with_box = img0\n",
    "for i in range(len(box_input_xywh)):\n",
    "    if box_labels[i] == 1:\n",
    "        color = (0, 255, 0)\n",
    "    else:\n",
    "        color = (255, 0, 0)\n",
    "    image_with_box = draw_box_on_image(image_with_box, box_input_xywh[i], color)\n",
    "plt.imshow(image_with_box)\n",
    "plt.axis(\"off\")  # Hide the axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 1352849976506927,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "plot_results(img0, inference_state)"
   ]
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "ff722971-ca5d-431f-8450-ccfea4ff0708",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
